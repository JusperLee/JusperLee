### Hey üëãüèΩ, I'm [Kai Li!](https://cslikai.cn) 

<br/>

![](https://img.shields.io/endpoint?url=https%3A%2F%2Fhits.dwyl.com%2FJusperLee%2FJusperLee.json%3Fcolor%3Dpink)&nbsp;&nbsp;&nbsp; ![](https://img.shields.io/github/followers/JusperLee?color=pink
)&nbsp;&nbsp;&nbsp; ![](https://img.shields.io/github/stars/JusperLee?logo=github&color=pink) 

<img alt="GIF" src="https://media.giphy.com/media/836HiJc7pgzy8iNXCn/giphy.gif" />

<br/>

My name is Kai Li (Chinese name: ÊùéÂáØ). I'm a second-year master student at Department of Computer Science and Technology, Tsinghua University, supervised by Prof. [Xiaolin Hu (ËÉ°ÊôìÊûó)](http://www.xlhu.cn/). I am also a member of [TSAIL Group](https://ml.cs.tsinghua.edu.cn/) directed by Prof. [Bo Zhang (Âº†Êã®)](https://www.cs.tsinghua.edu.cn/info/1121/3552.htm) and Prof. [Jun zhu (Êú±ÂÜõ)](https://ml.cs.tsinghua.edu.cn/~jun/index.shtml). I am an intern at Tencent AI Lab, mainly doing research on causal speech separation, supervised by [Yi Luo (ÁΩóËâ∫)](https://scholar.google.com/citations?user=OSM9xooAAAAJ&hl=en).

ü§ó &nbsp; These works are open source to the best of my ability.

ü§ó &nbsp; I am currently doing research on multimodal speech separation, and am interested in other speech tasks (e.g., pre-training models and neuralscience). If you would like to collaborate, please contact me. Many thanks.

#### **:bookmark: Homepages**

![](https://img.shields.io/twitter/url?url=https%3A%2F%2Fwww.linkedin.com%2Fin%2F%25E5%2587%25AF-%25E6%259D%258E-0bb2451a4&logo=Linkedin&label=Kai%20Li)&nbsp;&nbsp;&nbsp; ![](https://img.shields.io/twitter/url?url=https%3A%2F%2Fwww.zhihu.com%2Fpeople%2Fli-kai-34-50&logo=Zhihu&label=Jusper%20Lee) &nbsp;&nbsp;&nbsp; ![](https://img.shields.io/website?url=https%3A%2F%2Fcslikai.cn%2F&logo=Homepage&label=Audiomack)

#### **:date: News**

#### **Selected Publications:**

See [Google Scholar](https://scholar.google.com.hk/citations?user=fHkHcMsAAAAJ&hl=en) for a full list of publications.

**Speech Separation**

- [An efficient encoder-decoder architecture with top-down attention for speech separation](https://arxiv.org/pdf/2209.15200). **Kai Li**, Runxuan Yang, Xiaolin Hu. **ICLR 2023**.
- [An Audio-Visual Speech Separation Model Inspired by Cortico-Thalamo-Cortical Circuits](https://arxiv.org/abs/2212.10744). **Kai Li**, Fenghua Xie, Hang Chen, Kexin Yuan, Xiaolin Hu. **Arxiv 2022**.
- [Speech Separation Using an Asynchronous Fully Recurrent Convolutional Neural Network](https://papers.nips.cc/paper/2021/file/be1bc7997695495f756312886f566110-Paper.pdf) Xiaolin Hu, **Kai Li**, Weiyi Zhang, Yi Luo, Jean-Marie Lemercier, Timo Gerkmann. **NeurIPS 2021**.

**Neuroscience**
- [Inferring mechanisms of auditory attentional modulation with deep neural networks](https://cslikai.cn/files/neural.pdf). Ting-Yu Kuo, Yuanda Liao, **Kai Li**, Bo Hong, Xiaolin Hu. **Neural Computation 2022**.

**Cloud Removal**

-[PMAA: A Progressive Multi-scale Attention Autoencoder Model for High-Performance Cloud Removal from Multi-temporal Satellite Imagery](https://arxiv.org/pdf/2303.16565.pdf). Xuechao Zou, **Kai Li**, Junliang Xing, Pin Tao#, Yachao Cui. **ECAI 2023**.

**Super Resolution**

- [A Survey of Single Image Super Resolution Reconstruction](https://cslikai.cn/files/A_Survey_of_Single_Image_Super_Resolution_Reconstr.pdf). **Kai Li**, Shenghao Yang, Runting Dong, Jianqiang Huang, Xiaoying Wang. **IET Image Processing 2020**.
- [Single Image Super-resolution Reconstruction of Enhanced Loss Function with Multi-GPU Training](https://cslikai.cn/files/Single_Image_Super-Resolution_Reconstruction_of_Enhanced_Loss_Function_with_Multi-GPU_Training.pdf). Jianqiang Huang, **Kai Li**, Xiaoying Wang. **ISPA 2019**.